"""CLI utility to build a Postgres table from a SQL definition and CSV data.

This script rebuilds the project from scratch to provide a focused workflow:

1. Connect to a Postgres database using a connection string.
2. Read an MS SQL styled CREATE TABLE script and convert it to Postgres.
3. Create the table in Postgres.
4. Load rows from a CSV file into the new table.

The conversion handles common data types and removes SQL Server specific syntax.
It is not a full SQL parser but aims to support typical schema files.
"""
from __future__ import annotations

import argparse
import csv
import re
from dataclasses import dataclass
from typing import Iterable, List, Sequence, Tuple

import psycopg2
from psycopg2 import sql
from psycopg2.extras import execute_values


@dataclass
class TableDefinition:
    """Represents a table definition parsed from SQL text."""

    schema: str
    name: str
    create_sql: str
    columns: List[str]


TYPE_MAPPING = {
    "NVARCHAR": "VARCHAR",
    "VARCHAR": "VARCHAR",
    "CHAR": "CHAR",
    "NCHAR": "CHAR",
    "TEXT": "TEXT",
    "NTEXT": "TEXT",
    "INT": "INTEGER",
    "INTEGER": "INTEGER",
    "BIGINT": "BIGINT",
    "SMALLINT": "SMALLINT",
    "TINYINT": "SMALLINT",
    "BIT": "BOOLEAN",
    "FLOAT": "DOUBLE PRECISION",
    "REAL": "REAL",
    "DECIMAL": "DECIMAL",
    "NUMERIC": "NUMERIC",
    "MONEY": "NUMERIC(19,4)",
    "SMALLMONEY": "NUMERIC(10,4)",
    "DATETIME": "TIMESTAMP",
    "DATETIME2": "TIMESTAMP",
    "SMALLDATETIME": "TIMESTAMP",
    "DATE": "DATE",
    "TIME": "TIME",
    "UNIQUEIDENTIFIER": "UUID",
    "VARBINARY": "BYTEA",
    "BINARY": "BYTEA",
}


def read_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as handle:
        return handle.read()
def convert_mssql_sql(sql_text: str, target_table: str | None = None) -> TableDefinition:
    """Convert MS SQL styled CREATE TABLE to a Postgres compatible one.

    This function is intentionally conservative and focuses on common patterns.
    It rewrites identifiers, data types, identity columns, and removes
    SQL Server specific table options.
    """

    cleaned = re.sub(r"--.*", "", sql_text)
    cleaned = re.sub(r"/\*.*?\*/", "", cleaned, flags=re.DOTALL)
    cleaned = cleaned.replace("\r", "")

    match = re.search(
        r"CREATE\s+TABLE\s+(?:\[(?P<schema>[^\]]+)\]|(?P<schema2>\w+))?\.?(?:\[(?P<table>[^\]]+)\]|(?P<table2>\w+))",
        cleaned,
        flags=re.IGNORECASE,
    )
    if not match:
        raise ValueError("Could not locate a CREATE TABLE statement in SQL file")

    schema = match.group("schema") or match.group("schema2") or "public"
    name = target_table or match.group("table") or match.group("table2")

    def replace_identifier(segment: str) -> str:
        return re.sub(r"\[(?P<ident>[^\]]+)\]", lambda m: f'"{m.group("ident")}"', segment)

    converted = replace_identifier(cleaned)

    for mssql, pg in TYPE_MAPPING.items():
        converted = re.sub(rf"\b{mssql}\b", pg, converted, flags=re.IGNORECASE)

    converted = re.sub(r"IDENTITY\s*\(\s*\d+\s*,\s*\d+\s*\)", "GENERATED BY DEFAULT AS IDENTITY", converted, flags=re.IGNORECASE)
    converted = re.sub(r"WITH\s*\(.*?\)", "", converted, flags=re.IGNORECASE | re.DOTALL)
    converted = re.sub(r"\bON\s+\[?PRIMARY\]?", "", converted, flags=re.IGNORECASE)
    converted = re.sub(r"GO\s*$", "", converted, flags=re.IGNORECASE | re.MULTILINE)

    identifier_pattern = re.compile(re.escape(match.group(0)), flags=re.IGNORECASE)
    qualified_name = sql.Identifier(schema).string + "." + sql.Identifier(name).string
    converted = identifier_pattern.sub(f"CREATE TABLE {qualified_name}", converted, count=1)

    columns_block_match = re.search(r"CREATE\s+TABLE[^\(]+\((.*)\)\s*;?", converted, flags=re.IGNORECASE | re.DOTALL)
    if not columns_block_match:
        raise ValueError("Unable to parse column definitions from SQL file")

    columns_block = columns_block_match.group(1)
    columns: List[str] = []
    for raw_line in columns_block.split("\n"):
        line = raw_line.strip().rstrip(',')
        if not line or line.upper().startswith(("PRIMARY KEY", "CONSTRAINT", "FOREIGN KEY", "UNIQUE", "CHECK")):
            continue
        col_match = re.match(r"\"?([\w\d_]+)\"?", line)
        if col_match:
            columns.append(col_match.group(1))

    return TableDefinition(schema=schema, name=name, create_sql=converted.strip(), columns=columns)


def load_csv_rows(csv_path: str, columns: Sequence[str]) -> Iterable[Tuple]:
    with open(csv_path, newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        missing = [col for col in columns if col not in reader.fieldnames]
        if missing:
            raise ValueError(f"CSV file is missing required columns: {', '.join(missing)}")
        for row in reader:
            yield tuple(row.get(col) for col in columns)


def create_table(connection, definition: TableDefinition) -> None:
    with connection.cursor() as cursor:
        cursor.execute(sql.SQL("DROP TABLE IF EXISTS {}.{} CASCADE").format(
            sql.Identifier(definition.schema), sql.Identifier(definition.name)
        ))
        cursor.execute(definition.create_sql)
    connection.commit()


def insert_rows(connection, definition: TableDefinition, rows: Iterable[Tuple]) -> None:
    table_ident = sql.SQL("{}.{}".format(
        sql.Identifier(definition.schema).string,
        sql.Identifier(definition.name).string,
    ))
    columns_sql = sql.SQL(', ').join(sql.Identifier(c) for c in definition.columns)
    insert_sql = sql.SQL("INSERT INTO {} ({}) VALUES %s").format(table_ident, columns_sql)

    with connection.cursor() as cursor:
        execute_values(cursor, insert_sql.as_string(connection), rows, page_size=500)
    connection.commit()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Create a Postgres table from MS SQL schema and CSV data")
    parser.add_argument("--conn", required=True, help="Postgres connection string, e.g. postgres://user:pass@host:5432/db")
    parser.add_argument("--csv", required=True, help="Path to the CSV file containing data")
    parser.add_argument("--sql", required=True, help="Path to the MS SQL styled CREATE TABLE file")
    parser.add_argument("--table", help="Override the target table name")
    parser.add_argument("--schema", default="public", help="Target schema (defaults to public)")
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    sql_text = read_file(args.sql)
    definition = convert_mssql_sql(sql_text, target_table=args.table)
    if args.schema:
        definition.schema = args.schema

    print(f"Creating table {definition.schema}.{definition.name}...")
    conn = psycopg2.connect(args.conn)
    try:
        create_table(conn, definition)
        print("Table created.")

        print("Loading CSV rows...")
        rows = list(load_csv_rows(args.csv, definition.columns))
        insert_rows(conn, definition, rows)
        print(f"Inserted {len(rows)} rows.")
    finally:
        conn.close()


if __name__ == "__main__":
    main()
